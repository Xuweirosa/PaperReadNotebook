### DataMix: Efficient Privacy-Preserving Edge-Cloud Inference

#### Abstract

Deep neural network can be run either:
- locally (e.g. mobile phone)
- remotely (send to cloud)

However, either way has constraints.
- local - hardware restrictions
- cloud - privacy problems

This paper introduces DataMix (privacy-preserving edge-cloud inference framework).

- complete the majority of computation on cloud.
- do mixing and de-mixing operations for data updated to cloud in order to protect privacy.

#### 1. Introduction

Focus on the following performance:
- efficiency
- accuracy
- privacy

#### 2. Related Work

##### Efficient Inference

Efforts have been made by other previous researchers:
- create new efficient models (e.g. SqueezeNets)
- compress & accelerate existing models

##### Privacy-Preserving Inference
Privacy-Preserving: perform the inference on the cloud without leaking any private information

##### Hybrid Edge-Cloud Inference
- Some people send the data from very deep layers of neural network to cloud in order to protect privacy. But this means doing a lot of computation on local platforms.
- Not only the input privacy is important. The ouput privacy is also very important. We want to hide the output data under some situation.

#### 3. A Motivating Example

For example, we take two pictures $A$ and $B$, where $A$ is a dog picture and $B$ is a cat picture.

Here we can mix these two pictures with coefficients 
$\textbf{c}=[0.7,0.3]^T$, $\textbf{c'}=[0.6,0.4]^T$

$$\textbf{m(c)} = [A,B] \cdot \textbf{c} = 0.7A+0.3B$$

$$\textbf{m(c')} = [A,B] \cdot \textbf{c'} = 0.6A+0.4B$$

$$[\tilde{y_A},\tilde{y_B}] = [\tilde{y}_{m(c)},\tilde{y}_{m(c')}] \cdot C^{-1}$$

where $C^{-1} = [c,c']$

Here we only send the mixup to the cloud, and transmit only mixed inputs and outputs for model inference. Attackers on the network and cloud cannot access the coefficients, therefore they cannot recover the original private data from the mixed inputs and outputs. So this method can preserve private information. 

Also, the method is not locally-computational.

However, though it can protect original data from being recovered, some private information can still be recognized. In the paper, here is an example where from the mixed pictures we can still identify the color of the cat.

So the following part extend the method to DataMix for a general privacy-preserving framework.


#### 4. Method

Following the order:
- describe problem setting
- extend mixing & de-mixing for DataMix with
    - larger group size
    - intermediate features processing
- analyze the design choice & provide techniques for practice

##### Problem Setting
(omitted, some basic assumptions to concretely define the problem)

##### DataMix

###### Larger Group for Mixing
Q: Why mixing images with random coefficents can hide information in the data?

A: The entropy of the mixed data increases when pixels from the two images are combined.

Therefore, if we enlarge the group of images for mixing, the entropy will become larger as more information is combined.
![avatar](/picture/datamix_groupsize.png)

###### Intermediate Features Mixing

extend mixing and de-mixing to intermediate features.
e.g. the output of the first convolution layer.

This method re-project raw data to another space that provides better mixtures for the suffix layers to model the patterns.

###### Formalization

We partition the neural network $M$ sequentially into three parts:

$$M = M_{post}^E \circ M_{main}^C \circ M_{pre}^E$$

$M_{post}^E$ represents the preprocess model.

$M_{main}^C$ represents the main model.

$M_{pre}^E$ represents the postprocess model.

![avatar](/picture/datamix_model.png)

Preprocess model runs with raw inputs and sends output (i.e. intermediate input) to cloud.
Cloud runs the main model and sends output (i.e. intermediate output).
Postprocess model runs to get the final output.

Attackers can only access to intermediate input rather than raw input.

Without mixing, it is possible for cloud to train a neural network to approximate the inverse function of the preprocess model if it is shallow. So it may recover the real input data through the neural network.

###### Mixing-Aware Training

A batch of training data is fet into the proprocess model. Then mix images in each group with coefficients randomly sampled from orthogonal matrix group. And do the next steps.

###### Inter-Group Shuffling

Shuffle the intermediate inputs across the groups before sending data to the cloud to achieve better privacy protection.


##### Design Analysis

analyze several properties to be a good privacy-preserving method

###### Non-Invertiblity

The method for data transmitted to the cloud should be non-invertible without the private keys.

Here coefficients are randomly generated by the users and kept private. Attackers may have opportunity in the training process where if the images are i.i.d. and have a mean of 0, if they average a large set of mixtures that contain the same raw image, the average will be the same raw image in expectation. However, in the testing / application process, one image only appear in one group. Attackers won't get success.


###### Compatibility

###### Efficiency

#### 4. Experiment

Two experiments:
- facial attribute classification
    - benchmarks: CeleA and LFWA
- keyword spotting
